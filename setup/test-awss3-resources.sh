#!/bin/bash
# Description: This script tests AWS S3 bucket security configurations, including
# server-side encryption enforcement, lifecycle rules, and access permissions.
# It verifies that the bucket is properly configured according to security best practices.
# The script also tests uploading files with different storage classes to validate
# storage tiering capabilities and permissions.

# Remove 'set -e' to prevent script from exiting on expected failures
# Instead, we'll handle errors in our run_test function

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

clear
../utils/display-banner.sh

# Display script purpose and ask for confirmation
echo -e "\n${BLUE}AWS S3 Bucket Security Test${NC}"
echo -e "This script will test your S3 bucket for proper security configurations including:"
echo -e "‚Ä¢ ${YELLOW}Server-side encryption enforcement${NC}"
echo -e "  - Tests if bucket requires encryption for all uploads"
echo -e "  - Verifies uploads without encryption are rejected"
echo -e ""
echo -e "‚Ä¢ ${YELLOW}Secure access permissions${NC}"
echo -e "  - Verifies basic read/list permissions"
echo -e "  - Tests download restrictions"
echo -e "  - Checks object tagging capabilities"
echo -e "  - Tests deletion restrictions"
echo -e ""
echo -e "‚Ä¢ ${YELLOW}Lifecycle rules and retention policies${NC}"
echo -e "  - Validates access to lifecycle configuration"
echo -e "  - Tests backup prefix paths (daily, weekly, monthly)"
echo -e "  - Verifies storage class transition functionality (with admin access)"
echo -e ""
echo -e "‚Ä¢ ${YELLOW}Public access block settings${NC}"
echo -e "  - Verifies bucket is protected against public access\n"
echo -e ""
echo -e "‚Ä¢ ${YELLOW}Storage class capabilities${NC}"
echo -e "  - Tests uploading files with different storage classes"
echo -e "  - Validates permissions for each storage tier"
echo -e "  - Confirms proper storage class assignment\n"

read -rp "Do you want to proceed with the test? (yes/no): " response
response=$(echo "$response" | tr '[:upper:]' '[:lower:]')

if [[ "$response" != "yes" ]]; then
    echo -e "${YELLOW}Operation cancelled by user.${NC}"
    exit 0
fi

# Function to display usage
function show_usage {
    echo "Usage: $0 --bucket=BUCKET_NAME --access-key=ACCESS_KEY --secret-key=SECRET_KEY --region=AWS_REGION [--admin-profile=PROFILE]"
    echo "  --bucket        : Name of the S3 bucket to test"
    echo "  --access-key    : Access key of the IAM user"
    echo "  --secret-key    : Secret key of the IAM user"
    echo "  --region        : AWS region where the bucket is located"
    echo "  --admin-profile : (Optional) AWS CLI profile with admin access for additional tests"
}

# Function to get input with validation
function get_input {
    local prompt=$1
    local var_name=$2
    local value=""
    
    while [ -z "$value" ]; do
        read -rp "$prompt: " value
        if [ -z "$value" ]; then
            echo -e "${RED}This field is required. Please try again.${NC}"
        fi
    done
    
    eval "$var_name='$value'"
}

# Function to confirm execution
function confirm_execution {
    local response=""
    while [[ "$response" != "yes" && "$response" != "no" ]]; do
        read -rp "Do you want to proceed with the test? (yes/no): " response
        response=$(echo "$response" | tr '[:upper:]' '[:lower:]')
        
        if [[ "$response" != "yes" && "$response" != "no" ]]; then
            echo -e "${YELLOW}Please answer with 'yes' or 'no'.${NC}"
        fi
    done
    
    if [[ "$response" == "no" ]]; then
        echo -e "${YELLOW}Operation cancelled by user.${NC}"
        exit 0
    fi
}

# Function to run tests - improved to prevent script from exiting on expected failures
function run_test {
    local test_name=$1
    local command=$2
    local expected_exit_code=${3:-0}
    
    echo -e "${YELLOW}Test:${NC} üß™ $test_name"
    echo -e "${YELLOW}Command:${NC} üîç $command"
    
    # Run the command and capture output regardless of success/failure
    ( eval "$command" > /tmp/test_output 2>&1 )
    local exit_code=$?
    
    if [ $exit_code -eq "$expected_exit_code" ]; then
        echo -e "${GREEN}‚úÖ PASSED${NC} (Exit code: $exit_code)"
    else
        echo -e "${RED}‚ùå FAILED${NC} (Expected exit code: $expected_exit_code, got: $exit_code)"
        echo -e "${RED}Error output:${NC}"
        cat /tmp/test_output
    fi
    echo
    
    # Always return true so the script continues regardless of test outcome
    return 0
}

# Function for tests where any failure is considered a success
function run_test_any_failure {
    local test_name=$1
    local command=$2
    
    echo -e "${YELLOW}Test:${NC} üß™ $test_name"
    echo -e "${YELLOW}Command:${NC} üîç $command"
    
    # Run the command and capture output regardless of success/failure
    ( eval "$command" > /tmp/test_output 2>&1 )
    local exit_code=$?
    
    if [ $exit_code -ne 0 ]; then
        echo -e "${GREEN}‚úÖ PASSED${NC} (Exit code: $exit_code - Any failure is considered success)"
    else
        echo -e "${RED}‚ùå FAILED${NC} (Expected non-zero exit code, got: $exit_code)"
        echo -e "${RED}Output:${NC}"
        cat /tmp/test_output
    fi
    echo
    
    # Always return true so the script continues regardless of test outcome
    return 0
}

# Function for cleanup tests that considers AccessDenied as success
function run_cleanup_test {
    local test_name=$1
    local command=$2
    
    echo -e "${YELLOW}Test:${NC} üß™ $test_name"
    echo -e "${YELLOW}Command:${NC} üîç $command"
    
    # Run the command and capture output regardless of success/failure
    ( eval "$command" > /tmp/test_output 2>&1 )
    local exit_code=$?
    
    # Check if the error contains "AccessDenied" which indicates proper security restrictions
    if [ $exit_code -eq 0 ]; then
        echo -e "${GREEN}‚úÖ PASSED${NC} (Exit code: $exit_code - Object deleted successfully)"
    elif grep -q "AccessDenied" /tmp/test_output; then
        echo -e "${GREEN}‚úÖ PASSED${NC} (Access denied - This indicates proper security restrictions are in place)"
        echo -e "${YELLOW}Details: $(grep 'AccessDenied' /tmp/test_output)${NC}"
    else
        echo -e "${RED}‚ùå FAILED${NC} (Exit code: $exit_code - Unknown error)"
        echo -e "${RED}Error output:${NC}"
        cat /tmp/test_output
    fi
    echo
    
    # Always return true so the script continues regardless of test outcome
    return 0
}

# Test function: Check AWS Credentials and Configuration
# Verifies:
# - AWS CLI is properly configured and can access credentials
# - AWS CLI version is installed and functioning
function test_aws_credentials_config {
    echo -e "${BLUE}=== üîë Checking AWS Credentials Setup ===${NC}"
    run_test "Check AWS CLI configuration" "aws configure list $PROFILE_ARG"
    run_test "Check AWS CLI version" "aws --version"
}

# Test function: Basic bucket operations
# Verifies:
# - The provided IAM user has list permissions on the bucket
# - The bucket exists and is accessible
function test_basic_bucket_operations {
    echo -e "${BLUE}=== üìã Testing Basic Bucket Operations ===${NC}"
    run_test "List bucket" "aws s3 ls s3://$BUCKET_NAME/ $PROFILE_ARG"
}

# Test function: Upload permissions and encryption enforcement
# Verifies:
# - The IAM user can upload objects with server-side encryption
# - Bucket policy prevents uploads without encryption (enforces encryption)
function test_upload_permissions {
    echo -e "${BLUE}=== üì§ Testing Upload Permissions ===${NC}"
    # Create test files
    echo "Test content" > /tmp/test-file.txt
    echo "Unencrypted test content" > /tmp/unencrypted-file.txt

    # Test upload with server-side encryption (should succeed)
    run_test "Upload with encryption üîí" "aws s3 cp /tmp/test-file.txt s3://$BUCKET_NAME/test-file.txt --sse AES256 $PROFILE_ARG"

    # Test upload without encryption (should fail)
    run_test "Upload without encryption (should fail) üîì" "aws s3 cp /tmp/unencrypted-file.txt s3://$BUCKET_NAME/unencrypted-file.txt $PROFILE_ARG" 1
}

# Test function: Download restrictions
# Verifies:
# - The IAM user does not have permissions to download objects
# - Access controls are properly restricting read access
function test_download_restrictions {
    echo -e "${BLUE}=== üì• Testing Download Restrictions ===${NC}"
    # Test download (should fail, but considered a success if it fails)
    run_test "Download file (should fail) üì•" "aws s3 cp s3://$BUCKET_NAME/test-file.txt /tmp/downloaded-file.txt $PROFILE_ARG" 1
}

# Test function: Object tagging permissions
# Verifies:
# - The IAM user can add tags to objects (metadata management)
# - The IAM user can read object tags
function test_object_tagging {
    echo -e "${BLUE}=== üè∑Ô∏è Testing Object Tagging ===${NC}"
    run_test "Add tags to object üè∑Ô∏è" "aws s3api put-object-tagging --bucket $BUCKET_NAME --key test-file.txt --tagging 'TagSet=[{Key=purpose,Value=testing}]' $PROFILE_ARG"
    run_test "Get object tags üè∑Ô∏è" "aws s3api get-object-tagging --bucket $BUCKET_NAME --key test-file.txt $PROFILE_ARG"
}

# Test function: Lifecycle configuration
# Verifies:
# - The bucket has lifecycle rules or properly restricts access to view them
# - Bucket lifecycle configuration is present or access is appropriately restricted
function test_lifecycle_configuration {
    echo -e "${BLUE}=== ‚è±Ô∏è Testing Lifecycle Configuration ===${NC}"
    
    echo -e "${YELLOW}Test:${NC} üß™ Get lifecycle configuration"
    echo -e "${YELLOW}Command:${NC} üîç aws s3api get-bucket-lifecycle-configuration --bucket $BUCKET_NAME"
    
    # Run the command and capture output regardless of success/failure
    ( aws s3api get-bucket-lifecycle-configuration --bucket "$BUCKET_NAME" > /tmp/test_output 2>&1 )
    local exit_code=$?
    
    if [ $exit_code -eq 0 ]; then
        echo -e "${GREEN}‚úÖ PASSED${NC} (Successfully retrieved lifecycle configuration)"
        echo -e "${YELLOW}Configuration preview:${NC}"
        head -n 10 /tmp/test_output | sed 's/^/    /'
        if [ $(wc -l < /tmp/test_output) -gt 10 ]; then
            echo "    ..."
        fi
    else
        if grep -q "AccessDenied" /tmp/test_output; then
            echo -e "${GREEN}‚úÖ PASSED${NC} (Access denied due to security restrictions, which is acceptable)"
        elif grep -q "NoSuchLifecycleConfiguration" /tmp/test_output; then
            echo -e "${GREEN}‚úÖ PASSED${NC} (No lifecycle configuration exists for this bucket)"
        else
            echo -e "${YELLOW}‚ö†Ô∏è WARNING${NC} (Failed to retrieve lifecycle configuration with unexpected error)"
            cat /tmp/test_output
        fi
    fi
    echo
}

# Test function: Backup directory structure
# Verifies:
# - The IAM user can upload to designated backup prefix paths (daily, weekly, monthly)
# - The bucket is properly structured for backup retention
function test_backup_directories {
    echo -e "${BLUE}=== üìÅ Testing Backup Directory Structure ===${NC}"
    run_test "Upload daily backup test file üìÜ" "aws s3 cp /tmp/test-file.txt s3://$BUCKET_NAME/daily/test-file.txt --sse AES256 $PROFILE_ARG"
    run_test "Upload weekly backup test file üìÖ" "aws s3 cp /tmp/test-file.txt s3://$BUCKET_NAME/weekly/test-file.txt --sse AES256 $PROFILE_ARG"
    run_test "Upload monthly backup test file üìÜ" "aws s3 cp /tmp/test-file.txt s3://$BUCKET_NAME/monthly/test-file.txt --sse AES256 $PROFILE_ARG"
}

# Test function: Deletion restrictions
# Verifies:
# - The IAM user does not have permissions to delete objects
# - Access controls properly restrict deletion operations
function test_deletion_restrictions {
    echo -e "${BLUE}=== üóëÔ∏è Testing Deletion Restrictions ===${NC}"
    run_test "Delete object (should fail) ‚ùå" "aws s3 rm s3://$BUCKET_NAME/test-file.txt $PROFILE_ARG" 1
}

# Test function: Public access block
# Verifies:
# - The bucket has public access blocks configured or properly restricts viewing this setting
# - S3 bucket is protected against public access
function test_public_access_block {
    echo -e "${BLUE}=== üîí Testing Public Access Block ===${NC}"
    run_test_any_failure "Get public access block configuration (any failure is success)" "aws s3api get-public-access-block --bucket $BUCKET_NAME $PROFILE_ARG"
}

# Test function: Storage classes
# Verifies:
# - The IAM user can upload objects with different storage classes
# - Each storage class is properly assigned to the uploaded object
function test_storage_classes {
    # Only run this test if admin profile is provided and valid
    if [ -z "$ADMIN_PROFILE" ] || [ "$ADMIN_TESTS_SKIPPED" = true ]; then
        echo -e "${YELLOW}Skipping storage class tests as they require admin access.${NC}"
        return 0
    fi
    
    echo -e "${BLUE}=== üíæ Testing Storage Classes ===${NC}"
    
    # Create test file
    echo "Test content for storage class testing" > /tmp/storage-class-test.txt
    
    # Array of storage classes to test
    declare -a storage_classes=(
        "STANDARD"
        "STANDARD_IA"
        "ONEZONE_IA"
        "INTELLIGENT_TIERING"
        "GLACIER"
        "GLACIER_IR"
        "DEEP_ARCHIVE"
        "REDUCED_REDUNDANCY"
    )
    
    # Map of storage class descriptions
    declare -A descriptions=(
        ["STANDARD"]="Standard (default)"
        ["STANDARD_IA"]="Infrequent Access"
        ["ONEZONE_IA"]="One Zone Infrequent Access"
        ["INTELLIGENT_TIERING"]="Intelligent Tiering"
        ["GLACIER"]="Glacier Flexible Retrieval"
        ["GLACIER_IR"]="Glacier Instant Retrieval"
        ["DEEP_ARCHIVE"]="Glacier Deep Archive"
        ["REDUCED_REDUNDANCY"]="Reduced Redundancy (not recommended)"
    )
    
    echo -e "${YELLOW}Testing uploads with different storage classes to 'custom/' prefix...${NC}"
    
    for class in "${storage_classes[@]}"
    do
        # Create a file name based on the storage class
        local filename="custom/test-${class,,}.txt"  # Convert class to lowercase for filename
        
        echo -e "${YELLOW}Testing ${class} storage class: ${descriptions[$class]} üíæ${NC}"
        
        # Try to upload with the specific storage class
        run_test "Upload with ${class} storage class" "aws s3 cp /tmp/storage-class-test.txt s3://$BUCKET_NAME/$filename --storage-class $class --sse AES256 $PROFILE_ARG"
        
        # Special handling for STANDARD storage class which may not appear in output
        if [ "$class" = "STANDARD" ]; then
            echo -e "${YELLOW}Test:${NC} üß™ Verify ${class} storage class"
            echo -e "${YELLOW}Command:${NC} üîç aws s3api head-object --bucket $BUCKET_NAME --key $filename"
            
            # Run the command and capture output
            aws s3api head-object --bucket "$BUCKET_NAME" --key "$filename" > /tmp/storage_class_output 2>&1
            local exit_code=$?
            
            if [ $exit_code -eq 0 ]; then
                # Check if StorageClass is missing or STANDARD
                if ! grep -q "StorageClass" /tmp/storage_class_output || grep -q "StorageClass.*STANDARD" /tmp/storage_class_output; then
                    echo -e "${GREEN}‚úÖ PASSED${NC} (StorageClass is STANDARD or default)"
                else
                    echo -e "${RED}‚ùå FAILED${NC} (Expected default or STANDARD storage class)"
                    cat /tmp.storage_class_output
                fi
            else
                echo -e "${RED}‚ùå FAILED${NC} (Failed to get object details)"
                cat /tmp/storage_class_output
            fi
            echo
        else
            # For other storage classes, expect an exact match
            run_test "Verify ${class} storage class" "aws s3api head-object --bucket $BUCKET_NAME --key $filename | grep 'StorageClass' | grep '$class'"
        fi
    done
}

# Test function: Admin-specific tests
# Verifies:
# - Admin profile can access additional bucket settings
# - Bucket policy enforces encryption and secure transport
# - Storage class transitions are functioning correctly
function test_admin_capabilities {
    if [ -z "$ADMIN_PROFILE" ]; then
        echo -e "${YELLOW}Skipping admin tests as no admin profile was provided.${NC}"
        return 0
    fi
    
    echo -e "${BLUE}=== üîë Verifying Admin Profile ===${NC}"
    echo -e "${YELLOW}Checking if admin profile '$ADMIN_PROFILE' exists...${NC}"
    
    # Store current credentials to restore later
    local OLD_AWS_ACCESS_KEY_ID="$AWS_ACCESS_KEY_ID"
    local OLD_AWS_SECRET_ACCESS_KEY="$AWS_SECRET_ACCESS_KEY"
    local OLD_AWS_DEFAULT_REGION="$AWS_DEFAULT_REGION"
    
    # First verify the profile exists by trying to use it
    if ! aws s3 ls --profile "$ADMIN_PROFILE" > /dev/null 2>/tmp/profile_check_error; then
        echo -e "${RED}‚ùå ERROR: Could not use the admin profile '$ADMIN_PROFILE'. Error:${NC}"
        cat /tmp/profile_check_error
        echo -e "${RED}Admin-specific tests will be skipped.${NC}"
        ADMIN_TESTS_SKIPPED=true
        return 1
    fi
    
    # Profile exists, now try to get credentials
    local admin_access_key=""
    local admin_secret_key=""
    local admin_region=""
    
    # Try multiple methods to get credentials
    # Method 1: Using AWS CLI configure get
    echo -e "${YELLOW}Getting credentials from profile...${NC}"
    admin_access_key=$(aws configure get aws_access_key_id --profile "$ADMIN_PROFILE" 2>/dev/null)
    admin_secret_key=$(aws configure get aws_secret_access_key --profile "$ADMIN_PROFILE" 2>/dev/null)
    admin_region=$(aws configure get region --profile "$ADMIN_PROFILE" 2>/dev/null || echo "$REGION")
    
    # Method 2: If method 1 fails, try parsing from credentials file
    if [ -z "$admin_access_key" ] || [ -z "$admin_secret_key" ]; then
        echo -e "${YELLOW}Trying alternative method to get credentials...${NC}"
        if [ -f "$HOME/.aws/credentials" ]; then
            # Extract credentials using sed/grep if credentials file exists
            admin_access_key=$(grep -A3 "\[$ADMIN_PROFILE\]" "$HOME/.aws/credentials" | grep aws_access_key_id | cut -d '=' -f2 | tr -d ' ' 2>/dev/null)
            admin_secret_key=$(grep -A3 "\[$ADMIN_PROFILE\]" "$HOME/.aws/credentials" | grep aws_secret_access_key | cut -d '=' -f2 | tr -d ' ' 2>/dev/null)
        fi
        # Try to get region from config file if not found earlier
        if [ -z "$admin_region" ] && [ -f "$HOME/.aws/config" ]; then
            admin_region=$(grep -A3 "\[profile $ADMIN_PROFILE\]" "$HOME/.aws/config" | grep region | cut -d '=' -f2 | tr -d ' ' 2>/dev/null)
        fi
    fi
    
    # Method 3: If all else fails but the profile works, use temporary session credentials
    if ([ -z "$admin_access_key" ] || [ -z "$admin_secret_key" ]) && aws sts get-caller-identity --profile "$ADMIN_PROFILE" > /dev/null 2>&1; then
        echo -e "${YELLOW}Using AWS STS to get temporary credentials...${NC}"
        # Export AWS_PROFILE and use it directly in commands instead of trying to extract credentials
        export AWS_PROFILE="$ADMIN_PROFILE"
        export AWS_DEFAULT_REGION="${admin_region:-$REGION}"
        
        echo -e "${GREEN}‚úÖ Admin profile verified and set as active profile.${NC}"
        echo -e "${BLUE}=== üëë Running Additional Tests with Admin Access ===${NC}"
        
        # Test bucket policy enforcement (any failure is considered success)
        run_test_any_failure "Verify bucket policy (any failure is success) üìú" "aws s3api get-bucket-policy --bucket $BUCKET_NAME | grep -E 'DenyUnencryptedUploads|EnforceSecureTransport'"
        
        # Simulate lifecycle rules by modifying object storage class
        echo -e "${YELLOW}Testing storage class transitions (simulation)... üîÑ${NC}"
        
        # Test transition to Standard-IA
        run_test "Copy with Standard-IA storage class üì¶" "aws s3 cp /tmp/test-file.txt s3://$BUCKET_NAME/transition-test-ia.txt --storage-class STANDARD_IA --sse AES256"
        run_test "Verify Standard-IA storage class üîç" "aws s3api head-object --bucket $BUCKET_NAME --key transition-test-ia.txt | grep StorageClass | grep STANDARD_IA"
        
        # Test transition to Glacier
        run_test "Copy with Glacier storage class ‚ùÑÔ∏è" "aws s3 cp /tmp/test-file.txt s3://$BUCKET_NAME/transition-test-glacier.txt --storage-class GLACIER --sse AES256"
        run_test "Verify Glacier storage class üîç" "aws s3api head-object --bucket $BUCKET_NAME --key transition-test-glacier.txt | grep StorageClass | grep GLACIER"
        
        # Now run the dedicated storage class tests
        test_storage_classes
        
        # Restore original environment
        unset AWS_PROFILE
        export AWS_ACCESS_KEY_ID="$OLD_AWS_ACCESS_KEY_ID"
        export AWS_SECRET_ACCESS_KEY="$OLD_AWS_SECRET_ACCESS_KEY"
        export AWS_DEFAULT_REGION="$OLD_AWS_DEFAULT_REGION"
        
        return 0
    elif [ -n "$admin_access_key" ] && [ -n "$admin_secret_key" ]; then
        # We successfully got the credentials using method 1 or 2
        echo -e "${GREEN}‚úÖ Admin profile credentials retrieved successfully.${NC}"
        
        # Set AWS environment variables for admin
        export AWS_ACCESS_KEY_ID="$admin_access_key"
        export AWS_SECRET_ACCESS_KEY="$admin_secret_key"
        export AWS_DEFAULT_REGION="${admin_region:-$REGION}"
        
        echo -e "${BLUE}=== üëë Running Additional Tests with Admin Access ===${NC}"
        
        # Test bucket policy enforcement (any failure is considered success)
        run_test_any_failure "Verify bucket policy (any failure is success) üìú" "aws s3api get-bucket-policy --bucket $BUCKET_NAME | grep -E 'DenyUnencryptedUploads|EnforceSecureTransport'"
        
        # Simulate lifecycle rules by modifying object storage class
        echo -e "${YELLOW}Testing storage class transitions (simulation)... üîÑ${NC}"
        
        # Test transition to Standard-IA
        run_test "Copy with Standard-IA storage class üì¶" "aws s3 cp /tmp/test-file.txt s3://$BUCKET_NAME/transition-test-ia.txt --storage-class STANDARD_IA --sse AES256"
        run_test "Verify Standard-IA storage class üîç" "aws s3api head-object --bucket $BUCKET_NAME --key transition-test-ia.txt | grep StorageClass | grep STANDARD_IA"
        
        # Test transition to Glacier
        run_test "Copy with Glacier storage class ‚ùÑÔ∏è" "aws s3 cp /tmp/test-file.txt s3://$BUCKET_NAME/transition-test-glacier.txt --storage-class GLACIER --sse AES256"
        run_test "Verify Glacier storage class üîç" "aws s3api head-object --bucket $BUCKET_NAME --key transition-test-glacier.txt | grep StorageClass | grep GLACIER"
        
        # Now run the dedicated storage class tests
        test_storage_classes
        
        # Restore original credentials
        export AWS_ACCESS_KEY_ID="$OLD_AWS_ACCESS_KEY_ID"
        export AWS_SECRET_ACCESS_KEY="$OLD_AWS_SECRET_ACCESS_KEY"
        export AWS_DEFAULT_REGION="$OLD_AWS_DEFAULT_REGION"
        
        return 0
    else
        echo -e "${RED}‚ùå ERROR: Could not retrieve credentials from profile '$ADMIN_PROFILE'.${NC}"
        echo -e "${RED}Admin-specific tests will be skipped.${NC}"
        ADMIN_TESTS_SKIPPED=true
        return 1
    fi
}

# Function to fetch and display lifecycle configuration
# Verifies:
# - Retrieves and displays the bucket's lifecycle configuration for review
# - Parses rules for transitions and expirations for easier analysis
function show_lifecycle_configuration {
    echo -e "${BLUE}=== üìä Lifecycle Configuration Report ===${NC}"
    echo -e "${YELLOW}Retrieving bucket lifecycle configuration...${NC}"

    # Try to get lifecycle configuration using current credentials
    if ! aws s3api get-bucket-lifecycle-configuration --bucket "$BUCKET_NAME" > /tmp/lifecycle_config.json 2>/tmp/lifecycle_error; then
        echo -e "${YELLOW}Could not retrieve bucket lifecycle configuration:${NC}"
        cat /tmp/lifecycle_error
        echo -e "${YELLOW}This may be normal if no lifecycle rules are configured or you don't have permission to view them.${NC}"
    fi

    # Only continue if we have a valid lifecycle configuration file with content
    if [ -s /tmp/lifecycle_config.json ]; then
        # Parse the lifecycle configuration using jq if available, otherwise use grep
        if command -v jq >/dev/null 2>&1; then
            echo -e "${YELLOW}Storage Class Transitions:${NC}"
            jq -r '.Rules[] | select(.Transitions != null) | "Rule ID: \(.ID)\nStatus: \(.Status)\nTransitions: \(.Transitions | map("- After \(.TransitionInDays) days -> \(.StorageClass)") | join("\n"))"' /tmp/lifecycle_config.json 2>/dev/null || echo "No transition rules found or error parsing"
            
            echo -e "\n${YELLOW}Expiration Rules:${NC}"
            jq -r '.Rules[] | select(.ExpirationInDays != null) | "Rule ID: \(.ID)\nStatus: \(.Status)\nPrefix: \(.Prefix // "None")\nExpiration: After \(.ExpirationInDays) days"' /tmp/lifecycle_config.json 2>/dev/null || echo "No expiration rules found or error parsing"
        else
            echo -e "${YELLOW}Raw Lifecycle Configuration:${NC}"
            cat /tmp/lifecycle_config.json
        fi
    else
        echo -e "${YELLOW}No lifecycle configuration data available to display.${NC}"
    fi
}

# Function to ask for cleanup confirmation
function confirm_cleanup {
    echo
    echo -e "${RED}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó${NC}"
    echo -e "${RED}‚ïë                     CLEANUP CONFIRMATION                   ‚ïë${NC}"
    echo -e "${RED}‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù${NC}"
    echo -e "${YELLOW}The script will now remove all test files created in the S3 bucket:${NC}"
    echo -e "  ‚Ä¢ Basic test files (test-file.txt)"
    echo -e "  ‚Ä¢ Backup directory test files (daily/weekly/monthly)"
    
    if [ ! -z "$ADMIN_PROFILE" ] && [ "$ADMIN_TESTS_SKIPPED" = false ]; then
        echo -e "  ‚Ä¢ Storage class test files (under custom/ prefix)"
        echo -e "  ‚Ä¢ Transition test files (transition-test-ia.txt, transition-test-glacier.txt)"
    fi
    
    echo
    local response=""
    while [[ "$response" != "yes" && "$response" != "no" ]]; do
        read -rp "$(echo -e ${RED}"Do you want to proceed with cleanup? (yes/no): "${NC})" response
        response=$(echo "$response" | tr '[:upper:]' '[:lower:]')
        
        if [[ "$response" != "yes" && "$response" != "no" ]]; then
            echo -e "${YELLOW}Please answer with 'yes' or 'no'.${NC}"
        fi
    done
    
    if [[ "$response" == "no" ]]; then
        echo -e "${YELLOW}Cleanup cancelled by user. Test files will remain in the bucket.${NC}"
        return 1
    fi
    
    echo -e "${GREEN}Proceeding with cleanup...${NC}"
    return 0
}

# Function to cleanup test files
# Verifies:
# - Admin or IAM user can delete test objects or confirms deletion is restricted
# - Proper security measures are in place for cleanup operations
function cleanup_test_files {
    echo -e "${BLUE}=== üßπ Cleanup ===${NC}"
    
    # First ask for confirmation before proceeding with cleanup
    if ! confirm_cleanup; then
        return 0
    fi
    
    # For cleanup, only use admin credentials if they were verified
    local CURRENT_ACCESS_KEY="$AWS_ACCESS_KEY_ID"
    local CURRENT_SECRET_KEY="$AWS_SECRET_ACCESS_KEY"
    local CURRENT_REGION="$AWS_DEFAULT_REGION"
    
    if [ ! -z "$ADMIN_PROFILE" ] && [ "$ADMIN_TESTS_SKIPPED" = false ]; then
        # Try to use profile directly instead of extracting credentials
        if aws s3 ls --profile "$ADMIN_PROFILE" > /dev/null 2>&1; then
            echo -e "${YELLOW}Using admin profile for cleanup...${NC}"
            # Save the current profile if any
            local OLD_AWS_PROFILE="$AWS_PROFILE"
            export AWS_PROFILE="$ADMIN_PROFILE"
            
            # Use the cleanup test function for all test files
            run_cleanup_test "Remove test file üóëÔ∏è" "aws s3 rm s3://$BUCKET_NAME/test-file.txt"
            run_cleanup_test "Remove daily test file üóëÔ∏è" "aws s3 rm s3://$BUCKET_NAME/daily/test-file.txt"
            run_cleanup_test "Remove weekly test file üóëÔ∏è" "aws s3 rm s3://$BUCKET_NAME/weekly/test-file.txt"
            run_cleanup_test "Remove monthly test file üóëÔ∏è" "aws s3 rm s3://$BUCKET_NAME/monthly/test-file.txt"
            
            # Clean up storage class test files
            echo -e "${YELLOW}Cleaning up storage class test files...${NC}"
            declare -a storage_classes=(
                "STANDARD"
                "STANDARD_IA"
                "ONEZONE_IA"
                "INTELLIGENT_TIERING"
                "GLACIER"
                "GLACIER_IR"
                "DEEP_ARCHIVE"
                "REDUCED_REDUNDANCY"
            )
            
            for class in "${storage_classes[@]}"
            do
                local filename="custom/test-${class,,}.txt"  # Convert class to lowercase for filename
                run_cleanup_test "Remove ${class} test file üóëÔ∏è" "aws s3 rm s3://$BUCKET_NAME/$filename"
            done
            
            # Clean up transition test files - fix: separate commands for each file
            run_cleanup_test "Remove transition IA test file üóëÔ∏è" "aws s3 rm s3://$BUCKET_NAME/transition-test-ia.txt"
            run_cleanup_test "Remove transition Glacier test file üóëÔ∏è" "aws s3 rm s3://$BUCKET_NAME/transition-test-glacier.txt"
            
            # Restore original profile
            if [ -n "$OLD_AWS_PROFILE" ]; then
                export AWS_PROFILE="$OLD_AWS_PROFILE"
            else
                unset AWS_PROFILE
            fi
            return 0
        else
            # Traditional method using credentials if profile method fails
            # Get admin credentials again
            local admin_access_key
            admin_access_key=$(aws configure get aws_access_key_id --profile "$ADMIN_PROFILE")
            
            local admin_secret_key
            admin_secret_key=$(aws configure get aws_secret_access_key --profile "$ADMIN_PROFILE")
            local admin_region
            admin_region=$(aws configure get region --profile "$ADMIN_PROFILE" || echo "$REGION")
            
            export AWS_ACCESS_KEY_ID="$admin_access_key"
            export AWS_SECRET_ACCESS_KEY="$admin_secret_key"
            export AWS_DEFAULT_REGION="${admin_region:-$REGION}"
        fi
    fi

    # Use the cleanup test function for all test files
    run_cleanup_test "Remove test file üóëÔ∏è" "aws s3 rm s3://$BUCKET_NAME/test-file.txt"
    run_cleanup_test "Remove daily test file üóëÔ∏è" "aws s3 rm s3://$BUCKET_NAME/daily/test-file.txt"
    run_cleanup_test "Remove weekly test file üóëÔ∏è" "aws s3 rm s3://$BUCKET_NAME/weekly/test-file.txt"
    run_cleanup_test "Remove monthly test file üóëÔ∏è" "aws s3 rm s3://$BUCKET_NAME/monthly/test-file.txt"
    
    # Clean up storage class test files - only if admin tests were run
    if [ ! -z "$ADMIN_PROFILE" ] && [ "$ADMIN_TESTS_SKIPPED" = false ]; then
        echo -e "${YELLOW}Cleaning up storage class test files...${NC}"
        declare -a storage_classes=(
            "STANDARD"
            "STANDARD_IA"
            "ONEZONE_IA"
            "INTELLIGENT_TIERING"
            "GLACIER"
            "GLACIER_IR"
            "DEEP_ARCHIVE"
            "REDUCED_REDUNDANCY"
        )
        
        for class in "${storage_classes[@]}"
        do
            local filename="custom/test-${class,,}.txt"  # Convert class to lowercase for filename
            run_cleanup_test "Remove ${class} test file üóëÔ∏è" "aws s3 rm s3://$BUCKET_NAME/$filename"
        done
        
        # Clean up transition test files - fix: separate commands for each file
        run_cleanup_test "Remove transition IA test file üóëÔ∏è" "aws s3 rm s3://$BUCKET_NAME/transition-test-ia.txt"
        run_cleanup_test "Remove transition Glacier test file üóëÔ∏è" "aws s3 rm s3://$BUCKET_NAME/transition-test-glacier.txt"
    fi

    # Restore original credentials
    export AWS_ACCESS_KEY_ID="$CURRENT_ACCESS_KEY"
    export AWS_SECRET_ACCESS_KEY="$CURRENT_SECRET_KEY" 
    export AWS_DEFAULT_REGION="$CURRENT_REGION"
}

# Function to display test summary
function show_test_summary {
    echo -e "${BLUE}=== üìù Test Summary ===${NC}"
    echo -e "The script tested the following aspects of your S3 bucket:"
    echo -e "1. ${GREEN}üîë Basic access permissions${NC} (list bucket, upload/download files)"
    echo -e "2. ${GREEN}üîí Server-side encryption enforcement${NC} (upload with/without encryption)"
    echo -e "3. ${GREEN}üè∑Ô∏è Tagging permissions${NC} (add/get tags)"
    echo -e "4. ${GREEN}üóëÔ∏è Deletion restrictions${NC} (attempt to delete objects)"
    echo -e "5. ${GREEN}‚è±Ô∏è Lifecycle configuration${NC} (get configuration, verify backup retention paths)"
    echo -e "6. ${GREEN}üîí Public access block${NC} (verify configuration)"
    echo -e "7. ${GREEN}üíæ Storage class capabilities${NC} (upload with different storage classes)"
    if [ ! -z "$ADMIN_PROFILE" ]; then
        if [ "$ADMIN_TESTS_SKIPPED" = true ]; then
            echo -e "8. ${RED}‚ùå Admin profile tests SKIPPED${NC} - The profile '$ADMIN_PROFILE' could not be found or is invalid"
        else
            echo -e "8. ${GREEN}üîÑ Storage class transitions${NC} (simulation by direct uploads)"
        fi
    fi

    echo -e "\n${BLUE}üìå Notes:${NC}"
    echo -e "- Some lifecycle policies can only be fully validated over time as they depend on object age"
    echo -e "- Run this test periodically to verify lifecycle rules are working as expected"
    echo -e "- Consider setting up automated monitoring for S3 lifecycle events"
    echo -e "- Storage class availability may vary by region"
}

# Check for required tools
if ! command -v aws &> /dev/null; then
    echo -e "${RED}Error: AWS CLI is not installed. Please install it first.${NC}"
    exit 1
fi

# Check for jq (optional but recommended for better output)
if ! command -v jq &> /dev/null; then
    echo -e "${YELLOW}Warning: jq is not installed. Some lifecycle report features will be limited.${NC}"
fi

# Parse command line arguments
for i in "$@"; do
    case $i in
        --bucket=*)
        BUCKET_NAME="${i#*=}"
        shift
        ;;
        --access-key=*)
        ACCESS_KEY="${i#*=}"
        shift
        ;;
        --secret-key=*)
        SECRET_KEY="${i#*=}"
        shift
        ;;
        --region=*)
        REGION="${i#*=}"
        shift
        ;;
        --admin-profile=*)
        ADMIN_PROFILE="${i#*=}"
        shift
        ;;
        *)
        # unknown option
        show_usage
        exit 1
        ;;
    esac
done

# Interactive parameter collection if not provided
if [ -z "$BUCKET_NAME" ] || [ -z "$ACCESS_KEY" ] || [ -z "$SECRET_KEY" ] || [ -z "$REGION" ]; then
    echo -e "${BLUE}Some required parameters are missing. Let's collect them interactively:${NC}"
    
    [ -z "$BUCKET_NAME" ] && get_input "Enter S3 bucket name" "BUCKET_NAME"
    [ -z "$ACCESS_KEY" ] && get_input "Enter IAM user access key" "ACCESS_KEY"
    [ -z "$SECRET_KEY" ] && get_input "Enter IAM user secret key" "SECRET_KEY"
    [ -z "$REGION" ] && get_input "Enter AWS region" "REGION"
    
    if [ -z "$ADMIN_PROFILE" ]; then
        read -rp "Enter admin profile (optional, press Enter to skip): " ADMIN_PROFILE
    fi
fi

# Display parameters and ask for confirmation
echo -e "\n${BLUE}=============== TEST PARAMETERS ===============${NC}"
echo -e "${BLUE}S3 Bucket:${NC} $BUCKET_NAME"
echo -e "${BLUE}Access Key:${NC} $ACCESS_KEY"
echo -e "${BLUE}Secret Key:${NC} ${SECRET_KEY:0:4}****${SECRET_KEY: -4}" # Show only first 4 and last 4 characters
echo -e "${BLUE}AWS Region:${NC} $REGION"
if [ ! -z "$ADMIN_PROFILE" ]; then
    echo -e "${BLUE}Admin Profile:${NC} $ADMIN_PROFILE"
else
    echo -e "${BLUE}Admin Profile:${NC} Not provided"
fi
echo -e "${BLUE}==============================================${NC}\n"

# Confirm execution
confirm_execution

echo -e "${BLUE}===========================================${NC}"
echo -e "${BLUE}S3 Bucket Security and Lifecycle Test Script${NC}"
echo -e "${BLUE}===========================================${NC}"
echo -e "${BLUE}Testing bucket:${NC} $BUCKET_NAME"
echo -e "${BLUE}Region:${NC} $REGION"
echo

# Create a temporary AWS credentials file for the IAM user
TEMP_CONFIG_DIR=$(mktemp -d)
trap 'rm -rf $TEMP_CONFIG_DIR' EXIT

# Set AWS credentials directly as environment variables instead of using profiles
export AWS_ACCESS_KEY_ID="$ACCESS_KEY"
export AWS_SECRET_ACCESS_KEY="$SECRET_KEY" 
export AWS_DEFAULT_REGION="$REGION"

# Empty PROFILE_ARG as we're not using profiles anymore
PROFILE_ARG=""

# Verify AWS credentials and bucket existence before proceeding with tests
echo -e "${BLUE}=== üîë Verifying AWS Credentials and Bucket ===${NC}"

# Test AWS credentials
echo -e "${YELLOW}Verifying AWS credentials...${NC}"
if ! aws sts get-caller-identity &>/tmp/aws_cred_test; then
    echo -e "${RED}‚ùå ERROR: Failed to authenticate with AWS using the provided credentials.${NC}"
    echo -e "${RED}Please check your access key and secret key.${NC}"
    cat /tmp/aws_cred_test
    exit 1
else
    echo -e "${GREEN}‚úÖ AWS credentials verified successfully.${NC}"
    aws sts get-caller-identity | grep "UserId"
fi

# Test bucket existence
echo -e "${YELLOW}Verifying bucket existence...${NC}"
if ! aws s3api head-bucket --bucket "$BUCKET_NAME" &>/tmp/bucket_test 2>&1; then
    echo -e "${RED}‚ùå ERROR: The bucket '$BUCKET_NAME' does not exist or you don't have access to it.${NC}"
    echo -e "${RED}Please check the bucket name and your permissions.${NC}"
    cat /tmp/bucket_test
    exit 1
else
    echo -e "${GREEN}‚úÖ Bucket '$BUCKET_NAME' exists and is accessible.${NC}"
fi

echo -e "${GREEN}‚úÖ Initial verification completed successfully. Proceeding with tests...${NC}\n"

# Run all the tests in sequence
ADMIN_TESTS_SKIPPED=false

# Execute the test functions
test_aws_credentials_config
test_basic_bucket_operations
test_upload_permissions
test_download_restrictions
test_object_tagging
test_lifecycle_configuration
test_backup_directories
test_deletion_restrictions
test_public_access_block
test_admin_capabilities
# Remove test_storage_classes from execution flow since it's now called from test_admin_capabilities
show_lifecycle_configuration
cleanup_test_files
show_test_summary

# Clean up temporary files
rm -f /tmp/test-file.txt /tmp/unencrypted-file.txt /tmp/downloaded-file.txt /tmp/lifecycle_config.json /tmp/storage-class-test.txt
